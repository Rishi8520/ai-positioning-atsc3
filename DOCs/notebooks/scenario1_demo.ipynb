{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866469e4",
   "metadata": {},
   "source": [
    "# Scenario 1 Demo: Urban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario1** (Seoul Urban).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario1\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c0b4a",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario1\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(validate_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "print(\"VALIDATION:\", \"PASSED\" if result.returncode == 0 else \"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84115c1b",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario1 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(eval_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d000ba",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        comparison_csv = None\n",
    "        print(\"No evaluation directories found\")\n",
    "else:\n",
    "    comparison_csv = None\n",
    "    print(f\"Output directory not found: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447755be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table (using csv module)\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Display key metrics\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954aa87",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val, optimised_val = 0.0, 0.0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    bars = ax.bar([\"Baseline\", \"Optimised\"], [baseline_val, optimised_val],\n",
    "                  color=[\"#3498db\", \"#2ecc71\"], edgecolor=\"black\", linewidth=1.2)\n",
    "    \n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 1: Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41849770",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files:**\n",
    "- `OUTPUTS/scenario1/evaluation/<timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario1/evaluation/<timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario1/evaluation/<timestamp>/intent_score_explain.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6977e",
   "metadata": {},
   "source": [
    "# Scenario 1 Demo: Urban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario1** (Seoul Urban).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d16f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario1\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259df14c",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario1\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "print(\"VALIDATION:\", \"PASSED\" if result.returncode == 0 else \"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310d7b4",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba09d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario1 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2592ae9",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    comparison_csv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# Using csv module (no external dependencies)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Display key metrics in a formatted table\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf479d",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=[\"#3498db\", \"#2ecc71\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 1: Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d467470",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/intent_score_explain.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf6d92",
   "metadata": {},
   "source": [
    "# Scenario 1 Demo: Urban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario1** (Seoul Urban).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario1\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13d893",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c16401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario1\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "print(\"VALIDATION:\", \"PASSED\" if result.returncode == 0 else \"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51ad66",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba847036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario1 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9561a19",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e274d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    comparison_csv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# Using csv module (no external dependencies)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Display key metrics in a formatted table\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df24593",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80401fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=[\"#3498db\", \"#2ecc71\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 1: Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1888edc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario1/evaluation/<eval_timestamp>/intent_score_explain.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
