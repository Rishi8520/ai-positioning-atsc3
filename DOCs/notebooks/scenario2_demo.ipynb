{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9da90f",
   "metadata": {},
   "source": [
    "# Scenario 2 Demo: Suburban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario2** (Suburban environment).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd76ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario2\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d155a",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario2 may fail strict validation if real RINEX data is not yet populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6520ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario2\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(validate_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "print(\"VALIDATION:\", \"PASSED\" if result.returncode == 0 else \"FAILED (may need real RINEX data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1efb4",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario2 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(eval_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48320d53",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        comparison_csv = None\n",
    "        print(\"No evaluation directories found\")\n",
    "else:\n",
    "    comparison_csv = None\n",
    "    print(f\"Output directory not found: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table (using csv module)\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff800d",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val, optimised_val = 0.0, 0.0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    bars = ax.bar([\"Baseline\", \"Optimised\"], [baseline_val, optimised_val],\n",
    "                  color=[\"#e74c3c\", \"#27ae60\"], edgecolor=\"black\", linewidth=1.2)\n",
    "    \n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 2: Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75da80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files:**\n",
    "- `OUTPUTS/scenario2/evaluation/<timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario2/evaluation/<timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario2/evaluation/<timestamp>/intent_score_explain.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf4571",
   "metadata": {},
   "source": [
    "# Scenario 2 Demo: Suburban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario2** (Suburban environment).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario2\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298d19e",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario2 may fail strict validation if real RINEX data is not yet populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario2\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "validation_passed = result.returncode == 0\n",
    "print(\"VALIDATION:\", \"PASSED\" if validation_passed else \"FAILED (may need real RINEX data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53c91e",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d005a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario2 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefac9a4",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    print(\"Run Step 2 first to generate outputs.\")\n",
    "    comparison_csv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d72d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# Using csv module (no external dependencies)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Display key metrics in a formatted table\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004f8be",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=[\"#e74c3c\", \"#27ae60\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 2 (Suburban): Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae977757",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/intent_score_explain.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304e048",
   "metadata": {},
   "source": [
    "# Scenario 2 Demo: Suburban RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario2** (Suburban environment).\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results as a table\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaca879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario2\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc2143",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario2 may fail strict validation if real RINEX data is not yet populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario2\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "validation_passed = result.returncode == 0\n",
    "print(\"VALIDATION:\", \"PASSED\" if validation_passed else \"FAILED (may need real RINEX data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111a347",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d85087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario2 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452391a",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c968f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    print(\"Run Step 2 first to generate outputs.\")\n",
    "    comparison_csv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a518cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# Using csv module (no external dependencies)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Display key metrics in a formatted table\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Simulated: {comparison_data.get('simulated', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff2efd",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46322fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=[\"#e74c3c\", \"#27ae60\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    ax.set_title(f\"Scenario 2 (Suburban): Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\", fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d433a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario2/evaluation/<eval_timestamp>/intent_score_explain.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
