{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d85206e",
   "metadata": {},
   "source": [
    "# Scenario 3 Demo: Highway/Rural RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario3** (Highway/Rural).\n",
    "\n",
    "**Important:** Scenario3 may use **simulated metrics** if real RINEX data is unavailable.\n",
    "This notebook explicitly checks and displays the `simulated` flag and provenance.\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results (with simulated flag check)\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario3\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cd41a",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario3 typically fails strict validation - it uses simulated/synthetic data.\n",
    "The evaluation will proceed with simulated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario3\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(validate_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "print(\"VALIDATION:\", \"PASSED\" if result.returncode == 0 else \"FAILED (expected - will use simulated metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196dc746",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e806aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario3 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(eval_cmd, cwd=str(GNSS_DIR), capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05bb23",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv (with Simulated Flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aaa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        intent_explain = latest_eval / \"intent_score_explain.json\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        comparison_csv = None\n",
    "        intent_explain = None\n",
    "        print(\"No evaluation directories found\")\n",
    "else:\n",
    "    comparison_csv = None\n",
    "    intent_explain = None\n",
    "    print(f\"Output directory not found: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv - CHECK SIMULATED FLAG\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break\n",
    "    \n",
    "    # Check simulated flag - PROMINENT DISPLAY\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # PROMINENT SIMULATED FLAG\n",
    "    if is_simulated:\n",
    "        print(\"\\n\" + \"*\" * 70)\n",
    "        print(\"*  WARNING: SIMULATED=TRUE                                            *\")\n",
    "        print(\"*  Metrics are from simulated/synthetic data.                        *\")\n",
    "        print(\"*  Results are for demonstration purposes only.                      *\")\n",
    "        print(\"*\" * 70 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"\\nSimulated: False (Real RINEX data used)\")\n",
    "    \n",
    "    print(f\"Data Source: {comparison_data.get('data_source', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10032b0",
   "metadata": {},
   "source": [
    "## Step 3b: Display Intent Score Provenance (Simulated Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc104fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display intent_score_explain.json which contains simulated note\n",
    "if intent_explain and intent_explain.exists():\n",
    "    with open(intent_explain, \"r\", encoding=\"utf-8\") as f:\n",
    "        explain_data = json.load(f)\n",
    "    \n",
    "    print(\"INTENT SCORE PROVENANCE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Intent: {explain_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Final Score: {explain_data.get('final_score', 'N/A')}\")\n",
    "    print(f\"Formula: {explain_data.get('formula', 'N/A')}\")\n",
    "    \n",
    "    # Check for simulated note - important for scenario3\n",
    "    simulated_note = explain_data.get('simulated_note')\n",
    "    if simulated_note:\n",
    "        print(\"\\n\" + \"!\" * 70)\n",
    "        print(f\"SIMULATED NOTE: {simulated_note}\")\n",
    "        print(\"!\" * 70)\n",
    "    \n",
    "    print(\"\\nComputation Steps:\")\n",
    "    for step in explain_data.get('computation_steps', []):\n",
    "        print(f\"  - {step}\")\n",
    "else:\n",
    "    print(\"intent_score_explain.json not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d323475",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f181699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val, optimised_val = 0.0, 0.0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    # Gray colors for simulated data, colored for real\n",
    "    colors = [\"#95a5a6\", \"#7f8c8d\"] if is_simulated else [\"#9b59b6\", \"#1abc9c\"]\n",
    "    \n",
    "    bars = ax.bar([\"Baseline\", \"Optimised\"], [baseline_val, optimised_val],\n",
    "                  color=colors, edgecolor=\"black\", linewidth=1.2)\n",
    "    \n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    title = f\"Scenario 3: Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\"\n",
    "    if is_simulated:\n",
    "        title += \"\\n[SIMULATED DATA]\"\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3805e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files:**\n",
    "- `OUTPUTS/scenario3/evaluation/<timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario3/evaluation/<timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario3/evaluation/<timestamp>/intent_score_explain.json`\n",
    "\n",
    "**Note on Simulated Data:**\n",
    "When `simulated=True`, metrics are computed using deterministic simulation based on\n",
    "scenario profile parameters, not actual RTKLIB processing of RINEX data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd11ee",
   "metadata": {},
   "source": [
    "# Scenario 3 Demo: Highway/Rural RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario3** (Highway/Rural environment).\n",
    "\n",
    "**Important:** Scenario3 may use **simulated metrics** if real RINEX data is unavailable.\n",
    "This notebook explicitly checks and displays the `simulated` flag and provenance.\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results (with simulated flag check)\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario3\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745717e1",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario3 typically fails strict validation because it uses simulated/synthetic data.\n",
    "This is expected - the evaluation will proceed with simulated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d448fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario3\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "validation_passed = result.returncode == 0\n",
    "print(\"VALIDATION:\", \"PASSED\" if validation_passed else \"FAILED (expected for scenario3 - will use simulated metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba7017",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario3 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce8956",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv (with Simulated Flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        comparison_json = latest_eval / \"comparison.json\"\n",
    "        intent_explain = latest_eval / \"intent_score_explain.json\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "        comparison_json = None\n",
    "        intent_explain = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    print(\"Run Step 2 first to generate outputs.\")\n",
    "    comparison_csv = None\n",
    "    comparison_json = None\n",
    "    intent_explain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# IMPORTANT: Check and display simulated flag prominently\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Check simulated flag\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    # Display header with prominent simulated warning\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # PROMINENT SIMULATED FLAG DISPLAY\n",
    "    if is_simulated:\n",
    "        print(\"\\n\" + \"*\" * 70)\n",
    "        print(\"*  WARNING: SIMULATED=TRUE                                            *\")\n",
    "        print(\"*  Metrics are computed from simulated/synthetic data.               *\")\n",
    "        print(\"*  Results are for demonstration purposes only.                      *\")\n",
    "        print(\"*\" * 70 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"\\nSimulated: {comparison_data.get('simulated', 'N/A')} (Real RINEX data used)\")\n",
    "    \n",
    "    print(f\"Data Source: {comparison_data.get('data_source', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328f356",
   "metadata": {},
   "source": [
    "## Step 3b: Display Intent Score Provenance (Simulated Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display intent_score_explain.json which contains simulated note\n",
    "if intent_explain and intent_explain.exists():\n",
    "    with open(intent_explain, \"r\", encoding=\"utf-8\") as f:\n",
    "        explain_data = json.load(f)\n",
    "    \n",
    "    print(\"INTENT SCORE PROVENANCE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Intent: {explain_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Final Score: {explain_data.get('final_score', 'N/A')}\")\n",
    "    print(f\"Formula: {explain_data.get('formula', 'N/A')}\")\n",
    "    \n",
    "    # Check for simulated note\n",
    "    simulated_note = explain_data.get('simulated_note')\n",
    "    if simulated_note:\n",
    "        print(\"\\n\" + \"!\" * 70)\n",
    "        print(f\"SIMULATED NOTE: {simulated_note}\")\n",
    "        print(\"!\" * 70)\n",
    "    \n",
    "    print(\"\\nComputation Steps:\")\n",
    "    for step in explain_data.get('computation_steps', []):\n",
    "        print(f\"  - {step}\")\n",
    "else:\n",
    "    print(\"intent_score_explain.json not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ae1f1",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    # Use different colors for simulated data\n",
    "    if is_simulated:\n",
    "        colors = [\"#95a5a6\", \"#7f8c8d\"]  # Gray tones for simulated\n",
    "    else:\n",
    "        colors = [\"#9b59b6\", \"#1abc9c\"]  # Purple/teal for real\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=colors,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    \n",
    "    # Title includes simulated warning\n",
    "    title = f\"Scenario 3 (Highway/Rural): Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\"\n",
    "    if is_simulated:\n",
    "        title += \"\\n[SIMULATED DATA]\"\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc803c6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/intent_score_explain.json`\n",
    "\n",
    "**Note on Simulated Data:**\n",
    "When `simulated=True`, metrics are computed using deterministic simulation based on\n",
    "scenario profile parameters, not actual RTKLIB processing of RINEX data.\n",
    "This is useful for pipeline testing but should not be used for production evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ed0a6",
   "metadata": {},
   "source": [
    "# Scenario 3 Demo: Highway/Rural RTK Evaluation\n",
    "\n",
    "This notebook demonstrates the GNSS RTK evaluation pipeline for **scenario3** (Highway/Rural environment).\n",
    "\n",
    "**Important:** Scenario3 may use **simulated metrics** if real RINEX data is unavailable.\n",
    "This notebook explicitly checks and displays the `simulated` flag and provenance.\n",
    "\n",
    "**Steps:**\n",
    "1. Validate scenario data with `--strict-real`\n",
    "2. Run RTK evaluation with intent scoring\n",
    "3. Display comparison results (with simulated flag check)\n",
    "4. Plot baseline vs optimised horizontal error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (adjust if running from different location)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "GNSS_DIR = PROJECT_ROOT / \"CODE\" / \"gnss\"\n",
    "SCENARIO = \"scenario3\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"GNSS module: {GNSS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e8658",
   "metadata": {},
   "source": [
    "## Step 1: Validate Scenario Data (Strict Real Mode)\n",
    "\n",
    "**Note:** Scenario3 typically fails strict validation because it uses simulated/synthetic data.\n",
    "This is expected - the evaluation will proceed with simulated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ce146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validate_scenario.py --strict-real scenario3\n",
    "validate_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"validate_scenario.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--strict-real\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(validate_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    validate_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")\n",
    "validation_passed = result.returncode == 0\n",
    "print(\"VALIDATION:\", \"PASSED\" if validation_passed else \"FAILED (expected for scenario3 - will use simulated metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477fe1c",
   "metadata": {},
   "source": [
    "## Step 2: Run RTK Evaluation with Intent Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ad2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rtk_evaluate.py --scenario scenario3 --intent accuracy\n",
    "eval_cmd = [\n",
    "    \"python3\", str(GNSS_DIR / \"rtk_evaluate.py\"),\n",
    "    \"--scenario\", SCENARIO,\n",
    "    \"--intent\", \"accuracy\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(eval_cmd)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_cmd,\n",
    "    cwd=str(GNSS_DIR),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(f\"\\nReturn code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4ec2e",
   "metadata": {},
   "source": [
    "## Step 3: Load and Display comparison.csv (with Simulated Flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest evaluation output directory\n",
    "output_root = PROJECT_ROOT / \"OUTPUTs\" / SCENARIO / \"evaluation\"\n",
    "\n",
    "if output_root.exists():\n",
    "    eval_dirs = sorted([d for d in output_root.iterdir() if d.is_dir()], reverse=True)\n",
    "    if eval_dirs:\n",
    "        latest_eval = eval_dirs[0]\n",
    "        comparison_csv = latest_eval / \"comparison.csv\"\n",
    "        comparison_json = latest_eval / \"comparison.json\"\n",
    "        intent_explain = latest_eval / \"intent_score_explain.json\"\n",
    "        print(f\"Latest evaluation: {latest_eval.name}\")\n",
    "        print(f\"Comparison CSV: {comparison_csv}\")\n",
    "    else:\n",
    "        print(\"No evaluation directories found\")\n",
    "        comparison_csv = None\n",
    "        comparison_json = None\n",
    "        intent_explain = None\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_root}\")\n",
    "    print(\"Run Step 2 first to generate outputs.\")\n",
    "    comparison_csv = None\n",
    "    comparison_json = None\n",
    "    intent_explain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bac212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display comparison.csv as a table\n",
    "# IMPORTANT: Check and display simulated flag prominently\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "if comparison_csv and comparison_csv.exists():\n",
    "    with open(comparison_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            comparison_data = dict(row)\n",
    "            break  # Single row\n",
    "    \n",
    "    # Check simulated flag\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    # Display header with prominent simulated warning\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCENARIO: {comparison_data.get('scenario', 'N/A')}\")\n",
    "    print(f\"Timestamp: {comparison_data.get('timestamp', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # PROMINENT SIMULATED FLAG DISPLAY\n",
    "    if is_simulated:\n",
    "        print(\"\\n\" + \"*\" * 70)\n",
    "        print(\"*  WARNING: SIMULATED=TRUE                                            *\")\n",
    "        print(\"*  Metrics are computed from simulated/synthetic data.               *\")\n",
    "        print(\"*  Results are for demonstration purposes only.                      *\")\n",
    "        print(\"*\" * 70 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"\\nSimulated: {comparison_data.get('simulated', 'N/A')} (Real RINEX data used)\")\n",
    "    \n",
    "    print(f\"Data Source: {comparison_data.get('data_source', 'N/A')}\")\n",
    "    print(f\"Intent: {comparison_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Intent Score: {comparison_data.get('intent_score', 'N/A')}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(f\"{'Metric':<35} {'Baseline':>15} {'Optimised':>15} {'Delta':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    metrics = [\n",
    "        (\"horizontal_error_rms_m\", \"HPE RMS (m)\"),\n",
    "        (\"horizontal_error_p95_m\", \"HPE P95 (m)\"),\n",
    "        (\"vertical_error_rms_m\", \"VPE RMS (m)\"),\n",
    "        (\"fix_rate_pct\", \"Fix Rate (%)\"),\n",
    "        (\"availability_pct\", \"Availability (%)\"),\n",
    "        (\"ttff_sec\", \"TTFF (sec)\"),\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics:\n",
    "        baseline_val = comparison_data.get(f\"baseline_{metric_key}\", \"\")\n",
    "        optimised_val = comparison_data.get(f\"optimised_{metric_key}\", \"\")\n",
    "        delta_key = metric_key.replace(\"_m\", \"_delta_m\").replace(\"_pct\", \"_delta_pct\").replace(\"_sec\", \"_delta_sec\")\n",
    "        delta_val = comparison_data.get(f\"delta_{delta_key}\", \"\")\n",
    "        print(f\"{metric_name:<35} {baseline_val:>15} {optimised_val:>15} {delta_val:>15}\")\n",
    "else:\n",
    "    print(\"comparison.csv not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e66d",
   "metadata": {},
   "source": [
    "## Step 3b: Display Intent Score Provenance (Simulated Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aaa23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display intent_score_explain.json which contains simulated note\n",
    "if intent_explain and intent_explain.exists():\n",
    "    with open(intent_explain, \"r\", encoding=\"utf-8\") as f:\n",
    "        explain_data = json.load(f)\n",
    "    \n",
    "    print(\"INTENT SCORE PROVENANCE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Intent: {explain_data.get('intent', 'N/A')}\")\n",
    "    print(f\"Final Score: {explain_data.get('final_score', 'N/A')}\")\n",
    "    print(f\"Formula: {explain_data.get('formula', 'N/A')}\")\n",
    "    \n",
    "    # Check for simulated note\n",
    "    simulated_note = explain_data.get('simulated_note')\n",
    "    if simulated_note:\n",
    "        print(\"\\n\" + \"!\" * 70)\n",
    "        print(f\"SIMULATED NOTE: {simulated_note}\")\n",
    "        print(\"!\" * 70)\n",
    "    \n",
    "    print(\"\\nComputation Steps:\")\n",
    "    for step in explain_data.get('computation_steps', []):\n",
    "        print(f\"  - {step}\")\n",
    "else:\n",
    "    print(\"intent_score_explain.json not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503f1db",
   "metadata": {},
   "source": [
    "## Step 4: Plot Baseline vs Optimised HPE RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if comparison_data:\n",
    "    # Extract HPE RMS values\n",
    "    baseline_hpe = comparison_data.get(\"baseline_horizontal_error_rms_m\", \"\")\n",
    "    optimised_hpe = comparison_data.get(\"optimised_horizontal_error_rms_m\", \"\")\n",
    "    is_simulated = comparison_data.get('simulated', 'False').lower() == 'true'\n",
    "    \n",
    "    # Convert to float (handle empty strings)\n",
    "    try:\n",
    "        baseline_val = float(baseline_hpe) if baseline_hpe else 0.0\n",
    "        optimised_val = float(optimised_hpe) if optimised_hpe else 0.0\n",
    "    except ValueError:\n",
    "        baseline_val = 0.0\n",
    "        optimised_val = 0.0\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    # Use different colors for simulated data\n",
    "    if is_simulated:\n",
    "        colors = [\"#95a5a6\", \"#7f8c8d\"]  # Gray tones for simulated\n",
    "    else:\n",
    "        colors = [\"#9b59b6\", \"#1abc9c\"]  # Purple/teal for real\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        [\"Baseline\", \"Optimised\"],\n",
    "        [baseline_val, optimised_val],\n",
    "        color=colors,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, [baseline_val, optimised_val]):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.4f}\" if val > 0 else \"N/A\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "    \n",
    "    ax.set_ylabel(\"Horizontal Position Error RMS (m)\", fontsize=12)\n",
    "    \n",
    "    # Title includes simulated warning\n",
    "    title = f\"Scenario 3 (Highway/Rural): Baseline vs Optimised HPE RMS\\nIntent: {comparison_data.get('intent', 'N/A')}\"\n",
    "    if is_simulated:\n",
    "        title += \"\\n[SIMULATED DATA]\"\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    \n",
    "    ax.set_ylim(0, max(baseline_val, optimised_val, 0.1) * 1.3)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c67569",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Output Files Location:**\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/comparison.csv`\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/comparison.json`\n",
    "- `OUTPUTS/scenario3/evaluation/<eval_timestamp>/intent_score_explain.json`\n",
    "\n",
    "**Note on Simulated Data:**\n",
    "When `simulated=True`, metrics are computed using deterministic simulation based on\n",
    "scenario profile parameters, not actual RTKLIB processing of RINEX data.\n",
    "This is useful for pipeline testing but should not be used for production evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
